{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7229af0-6856-4f16-8a62-9d90679195bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358ed2d-66d2-4361-aa4e-85293e287a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85268c68-f9ca-4be4-a90b-1c27e8a38d32",
   "metadata": {},
   "source": [
    "#### 1. Sentiment analysis\n",
    "#### Build a model that can analyze the sentiment of text data, such as customer reviews or social media posts. Use techniques like bag-of-words, word embeddings, or transformers to classify text as positive, negative, or neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf40a3e-0454-411d-a767-b859d56de44c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f101faa4-24b6-421e-9011-f4bb912472d5",
   "metadata": {},
   "source": [
    "##### Step-by-step guide for building an end-to-end sentiment analysis model :\n",
    "\n",
    "##### Data Collection\n",
    "##### Data Preprocessing\n",
    "##### Feature Extraction\n",
    "##### Model Training\n",
    "##### Model Evaluation\n",
    "##### Predictions on New Data\n",
    "##### I am using dataset of movie reviews for this example, and we'll use libraries like nltk, sklearn, and transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f25c53-2c8f-49ad-8d69-63b47b3bf805",
   "metadata": {},
   "source": [
    "#### Data Collection\n",
    "##### Using nltk library to download the movie reviews dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e92d25a3-7057-4fd2-bc38-1939a5f2d87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the happy bastard's quick movie review \\ndamn ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  plot : two teen couples go to a church party ,...       neg\n",
       "1  the happy bastard's quick movie review \\ndamn ...       neg\n",
       "2  it is movies like these that make a jaded movi...       neg\n",
       "3   \" quest for camelot \" is warner bros . ' firs...       neg\n",
       "4  synopsis : a mentally unstable man undergoing ...       neg"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "def load_movie_reviews():\n",
    "    reviews = []\n",
    "    for fileid in movie_reviews.fileids():\n",
    "        category = movie_reviews.categories(fileid)[0]\n",
    "        review = movie_reviews.raw(fileid)\n",
    "        reviews.append((review, category))\n",
    "    return pd.DataFrame(reviews, columns=['review', 'sentiment'])\n",
    "\n",
    "df = load_movie_reviews()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8027cf55-7d29-470b-8332-768fbf78d0e1",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "##### Next, we'll preprocess the text data. This step includes tokenization, removing stopwords, and converting text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9caff38-a93b-424d-acea-a581a5e27880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot : two teen couples go to a church party ,...</td>\n",
       "      <td>neg</td>\n",
       "      <td>plot two teen couples go church party drink dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the happy bastard's quick movie review \\ndamn ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>happy bastard quick movie review damn bug got ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>it is movies like these that make a jaded movi...</td>\n",
       "      <td>neg</td>\n",
       "      <td>movies like make jaded movie viewer thankful i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n",
       "      <td>neg</td>\n",
       "      <td>quest camelot warner bros first featurelength ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis : a mentally unstable man undergoing ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>synopsis mentally unstable man undergoing psyc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  plot : two teen couples go to a church party ,...       neg   \n",
       "1  the happy bastard's quick movie review \\ndamn ...       neg   \n",
       "2  it is movies like these that make a jaded movi...       neg   \n",
       "3   \" quest for camelot \" is warner bros . ' firs...       neg   \n",
       "4  synopsis : a mentally unstable man undergoing ...       neg   \n",
       "\n",
       "                                      cleaned_review  \n",
       "0  plot two teen couples go church party drink dr...  \n",
       "1  happy bastard quick movie review damn bug got ...  \n",
       "2  movies like make jaded movie viewer thankful i...  \n",
       "3  quest camelot warner bros first featurelength ...  \n",
       "4  synopsis mentally unstable man undergoing psyc...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Convert to lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # Remove punctuation and stopwords\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [word.translate(table) for word in tokens]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35caa7fc-bc72-4087-b66d-c5080637d9e6",
   "metadata": {},
   "source": [
    "##### Feature Extraction\n",
    "##### We'll use TF-IDF (Term Frequency-Inverse Document Frequency) for feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b48c7c0-a768-480b-afd5-0d137dc9ae20",
   "metadata": {},
   "source": [
    "#####  It can be defined as the calculation of how relevant a word in a series or corpus is to a text. The meaning increases proportionally to the number of times in the text a word appears but is compensated by the word frequency in the corpus (data-set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07df069c-0087-4c2b-b863-315d99981eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 5000) (2000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['cleaned_review']).toarray()\n",
    "y = df['sentiment'].apply(lambda x: 1 if x == 'pos' else 0).values\n",
    "\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021ad82-d199-41dd-988b-4ceb6f094346",
   "metadata": {},
   "source": [
    "#### Model Training\n",
    "##### train a simple logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2335ea60-5fec-4383-bb9f-e44d6ddfc18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83       199\n",
      "           1       0.83      0.84      0.83       201\n",
      "\n",
      "    accuracy                           0.83       400\n",
      "   macro avg       0.83      0.83      0.83       400\n",
      "weighted avg       0.83      0.83      0.83       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8437a5-36cb-4b19-a86b-c015feeafea2",
   "metadata": {},
   "source": [
    "##### Model Testing \n",
    "#### Predictions on New Data\n",
    "##### test the model on some new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cd586ac-07b4-47c6-a1bc-a6613532e927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I loved this movie, it was fantastic!\n",
      "Sentiment: positive\n",
      "\n",
      "Review: This was a terrible movie, I hated it.\n",
      "Sentiment: negative\n",
      "\n",
      "Review: It was an average movie, not too bad but not great either.\n",
      "Sentiment: negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(review):\n",
    "    cleaned_review = preprocess_text(review)\n",
    "    vectorized_review = vectorizer.transform([cleaned_review]).toarray()\n",
    "    prediction = model.predict(vectorized_review)[0]\n",
    "    return 'positive' if prediction == 1 else 'negative'\n",
    "\n",
    "# Example reviews\n",
    "reviews = [\n",
    "    \"I loved this movie, it was fantastic!\",\n",
    "    \"This was a terrible movie, I hated it.\",\n",
    "    \"It was an average movie, not too bad but not great either.\"\n",
    "]\n",
    "\n",
    "for review in reviews:\n",
    "    print(f'Review: {review}')\n",
    "    print(f'Sentiment: {predict_sentiment(review)}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf43a43f-205c-4434-8b74-75402af9e292",
   "metadata": {},
   "source": [
    "#### The Complete Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28cf1288-820b-4ac5-a8b1-85e425423be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83       199\n",
      "           1       0.83      0.84      0.83       201\n",
      "\n",
      "    accuracy                           0.83       400\n",
      "   macro avg       0.83      0.83      0.83       400\n",
      "weighted avg       0.83      0.83      0.83       400\n",
      "\n",
      "Review: I loved this movie, it was fantastic!\n",
      "Sentiment: positive\n",
      "\n",
      "Review: This was a terrible movie, I hated it.\n",
      "Sentiment: negative\n",
      "\n",
      "Review: It was an average movie, not too bad but not great either.\n",
      "Sentiment: negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Data Collection\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "import pandas as pd\n",
    "\n",
    "def load_movie_reviews():\n",
    "    reviews = []\n",
    "    for fileid in movie_reviews.fileids():\n",
    "        category = movie_reviews.categories(fileid)[0]\n",
    "        review = movie_reviews.raw(fileid)\n",
    "        reviews.append((review, category))\n",
    "    return pd.DataFrame(reviews, columns=['review', 'sentiment'])\n",
    "\n",
    "df = load_movie_reviews()\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [word.translate(table) for word in tokens]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "# 3. Feature Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['cleaned_review']).toarray()\n",
    "y = df['sentiment'].apply(lambda x: 1 if x == 'pos' else 0).values\n",
    "\n",
    "# 4. Model Training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 5. Predictions on New Data\n",
    "def predict_sentiment(review):\n",
    "    cleaned_review = preprocess_text(review)\n",
    "    vectorized_review = vectorizer.transform([cleaned_review]).toarray()\n",
    "    prediction = model.predict(vectorized_review)[0]\n",
    "    return 'positive' if prediction == 1 else 'negative'\n",
    "\n",
    "reviews = [\n",
    "    \"I loved this movie, it was fantastic!\",\n",
    "    \"This was a terrible movie, I hated it.\",\n",
    "    \"It was an average movie, not too bad but not great either.\"\n",
    "]\n",
    "\n",
    "for review in reviews:\n",
    "    print(f'Review: {review}')\n",
    "    print(f'Sentiment: {predict_sentiment(review)}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e81bba2-7cbc-40b6-9c27-5ea02fe66078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b5648f4-0ecf-403d-91e0-22369fe73dbd",
   "metadata": {},
   "source": [
    "#### Enhanching the sentiment analysis project by incorporating techniques like Bag-of-Words (BoW), Word Embeddings, and Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527184e-bd20-4b6a-85eb-6b42cab34b3c",
   "metadata": {},
   "source": [
    "##### 1. Bag-of-Words (BoW) Approach\n",
    "##### using the BoW approach with CountVectorizer from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835a6718-3819-4f8a-b356-19ebd760dd53",
   "metadata": {},
   "source": [
    "##### Bag of words is a text vectorization technique that converts the text into finite length vectors. \n",
    "##### The boW model is easy to implement and understand. Bag of words has few drawbacks, which can be overcome by using advanced techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9926376-13b8-4310-98c5-ab813dcde042",
   "metadata": {},
   "source": [
    "##### 1.1 Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f93f742-f9fb-4d39-9ed3-d9b0db2fd922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Load the dataset\n",
    "def load_movie_reviews():\n",
    "    reviews = []\n",
    "    for fileid in movie_reviews.fileids():\n",
    "        category = movie_reviews.categories(fileid)[0]\n",
    "        review = movie_reviews.raw(fileid)\n",
    "        reviews.append((review, category))\n",
    "    return pd.DataFrame(reviews, columns=['review', 'sentiment'])\n",
    "\n",
    "df = load_movie_reviews()\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [word.translate(table) for word in tokens]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bb47b44-00ab-4a5d-a6be-90ab5269f792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "from nltk.corpus import movie_reviews\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Load the dataset\n",
    "def load_movie_reviews():\n",
    "    reviews = []\n",
    "    for fileid in movie_reviews.fileids():\n",
    "        category = movie_reviews.categories(fileid)[0]\n",
    "        review = movie_reviews.raw(fileid)\n",
    "        reviews.append((review, category))\n",
    "    return pd.DataFrame(reviews, columns=['review', 'sentiment'])\n",
    "\n",
    "df = load_movie_reviews()\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Preprocess the text data\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [word.translate(table) for word in tokens]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67425a6e-f6b6-4367-8137-351770fb9946",
   "metadata": {},
   "source": [
    "##### 1.2 Feature Extraction using BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18b28c4d-c236-46bc-9dd2-250095863d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.82       199\n",
      "           1       0.82      0.83      0.82       201\n",
      "\n",
      "    accuracy                           0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Extract features using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['cleaned_review']).toarray()\n",
    "y = df['sentiment'].apply(lambda x: 1 if x == 'pos' else 0).values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f9d239-0a18-4992-989a-ee7ae1d10f0f",
   "metadata": {},
   "source": [
    "##### 1.3 Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58ff529f-22a0-42a7-a747-4e23f2524f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I loved this movie, it was fantastic!\n",
      "Sentiment: negative\n",
      "\n",
      "Review: This was a terrible movie, I hated it.\n",
      "Sentiment: negative\n",
      "\n",
      "Review: It was an average movie, not too bad but not great either.\n",
      "Sentiment: negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(review):\n",
    "    cleaned_review = preprocess_text(review)\n",
    "    vectorized_review = vectorizer.transform([cleaned_review]).toarray()\n",
    "    prediction = model.predict(vectorized_review)[0]\n",
    "    return 'positive' if prediction == 1 else 'negative'\n",
    "\n",
    "# Example reviews\n",
    "reviews = [\n",
    "    \"I loved this movie, it was fantastic!\",\n",
    "    \"This was a terrible movie, I hated it.\",\n",
    "    \"It was an average movie, not too bad but not great either.\"\n",
    "]\n",
    "\n",
    "for review in reviews:\n",
    "    print(f'Review: {review}')\n",
    "    print(f'Sentiment: {predict_sentiment(review)}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64dc67-40e9-4b1e-b2b8-f0a758b0e58a",
   "metadata": {},
   "source": [
    "##### 2. Word Embeddings Approach\n",
    "##### Using pre-trained GloVe embeddings to represent our text data.\n",
    "\n",
    "##### 2.1 Load GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1c17d57-cad2-4769-a343-3ab533768740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "glove_file = r'C:\\\\Users\\\\hp\\\\Documents\\\\internships\\\\Ignite\\\\glove.6B.100d.txt'  # Updated file path\n",
    "embeddings_index = load_glove_embeddings(glove_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330bb941-2947-45b8-9209-7478da5639e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfde9b58-065f-4820-8255-7da6d9872ff3",
   "metadata": {},
   "source": [
    "##### 2.2 Create Embedding Matrix and Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45fdee4f-2b86-475b-85be-8ca1350b097e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.64.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hp\\documents\\anaconda\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d5c446c-d31a-473e-b4a4-d1ffc39741c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Documents\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,500</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │           \u001b[38;5;34m1,500\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,500</span> (5.86 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,500\u001b[0m (5.86 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,500</span> (5.86 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,500\u001b[0m (5.86 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Assuming df is your DataFrame with 'cleaned_review' and 'sentiment' columns\n",
    "df = pd.DataFrame({\n",
    "    'cleaned_review': [\"This is a great movie\", \"I did not like the film\", \"An excellent watch\"],\n",
    "    'sentiment': [\"pos\", \"neg\", \"pos\"]\n",
    "})\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['cleaned_review'])\n",
    "sequences = tokenizer.texts_to_sequences(df['cleaned_review'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Pad sequences\n",
    "max_length = 100\n",
    "X = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "glove_file = r'C:\\Users\\hp\\Documents\\internships\\Ignite\\glove.6B.100d.txt'\n",
    "embeddings_index = load_glove_embeddings(glove_file)\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Prepare the labels\n",
    "y = df['sentiment'].apply(lambda x: 1 if x == 'pos' else 0).values\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1, \n",
    "                    output_dim=embedding_dim, \n",
    "                    weights=[embedding_matrix], \n",
    "                    input_length=max_length, \n",
    "                    trainable=False))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be454f61-c6da-4c76-a6fa-22f10aecb974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 0.5000 - loss: 0.6971 - val_accuracy: 0.0000e+00 - val_loss: 0.7149\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 546ms/step - accuracy: 0.5000 - loss: 0.6951 - val_accuracy: 0.0000e+00 - val_loss: 0.7285\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 1.0000 - loss: 0.6237 - val_accuracy: 0.0000e+00 - val_loss: 0.7278\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 1.0000 - loss: 0.5405 - val_accuracy: 0.0000e+00 - val_loss: 0.7270\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 1.0000 - loss: 0.5217 - val_accuracy: 0.0000e+00 - val_loss: 0.7224\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 1.0000 - loss: 0.4998 - val_accuracy: 0.0000e+00 - val_loss: 0.7121\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 1.0000 - loss: 0.4660 - val_accuracy: 0.0000e+00 - val_loss: 0.6942\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 0.4713 - val_accuracy: 1.0000 - val_loss: 0.6733\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 1.0000 - loss: 0.4364 - val_accuracy: 1.0000 - val_loss: 0.6581\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 1.0000 - loss: 0.3228 - val_accuracy: 1.0000 - val_loss: 0.6428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1839dd81c90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Assuming X and y are your features and labels\n",
    "model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaca6e4-a6cd-44f0-8029-eefd4530bb7c",
   "metadata": {},
   "source": [
    "##### 2.3 Building and Train the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27ee8682-d092-4a43-b51b-1626b91628d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Index:\n",
      " [('this', 1), ('is', 2), ('a', 3), ('great', 4), ('movie', 5), ('i', 6), ('did', 7), ('not', 8), ('like', 9), ('the', 10)]\n",
      "\n",
      "Shape of X: (3, 100)\n",
      "Sample values from X:\n",
      " [[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1\n",
      "   2  3  4  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  6  7\n",
      "   8  9 10 11]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0 12 13 14]]\n",
      "\n",
      "Sample GloVe embeddings:\n",
      " {'the': array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
      "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
      "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
      "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
      "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
      "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
      "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
      "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
      "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
      "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
      "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
      "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
      "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
      "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
      "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
      "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
      "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32), ',': array([-0.10767  ,  0.11053  ,  0.59812  , -0.54361  ,  0.67396  ,\n",
      "        0.10663  ,  0.038867 ,  0.35481  ,  0.06351  , -0.094189 ,\n",
      "        0.15786  , -0.81665  ,  0.14172  ,  0.21939  ,  0.58505  ,\n",
      "       -0.52158  ,  0.22783  , -0.16642  , -0.68228  ,  0.3587   ,\n",
      "        0.42568  ,  0.19021  ,  0.91963  ,  0.57555  ,  0.46185  ,\n",
      "        0.42363  , -0.095399 , -0.42749  , -0.16567  , -0.056842 ,\n",
      "       -0.29595  ,  0.26037  , -0.26606  , -0.070404 , -0.27662  ,\n",
      "        0.15821  ,  0.69825  ,  0.43081  ,  0.27952  , -0.45437  ,\n",
      "       -0.33801  , -0.58184  ,  0.22364  , -0.5778   , -0.26862  ,\n",
      "       -0.20425  ,  0.56394  , -0.58524  , -0.14365  , -0.64218  ,\n",
      "        0.0054697, -0.35248  ,  0.16162  ,  1.1796   , -0.47674  ,\n",
      "       -2.7553   , -0.1321   , -0.047729 ,  1.0655   ,  1.1034   ,\n",
      "       -0.2208   ,  0.18669  ,  0.13177  ,  0.15117  ,  0.7131   ,\n",
      "       -0.35215  ,  0.91348  ,  0.61783  ,  0.70992  ,  0.23955  ,\n",
      "       -0.14571  , -0.37859  , -0.045959 , -0.47368  ,  0.2385   ,\n",
      "        0.20536  , -0.18996  ,  0.32507  , -1.1112   , -0.36341  ,\n",
      "        0.98679  , -0.084776 , -0.54008  ,  0.11726  , -1.0194   ,\n",
      "       -0.24424  ,  0.12771  ,  0.013884 ,  0.080374 , -0.35414  ,\n",
      "        0.34951  , -0.7226   ,  0.37549  ,  0.4441   , -0.99059  ,\n",
      "        0.61214  , -0.35111  , -0.83155  ,  0.45293  ,  0.082577 ],\n",
      "      dtype=float32), '.': array([-0.33979  ,  0.20941  ,  0.46348  , -0.64792  , -0.38377  ,\n",
      "        0.038034 ,  0.17127  ,  0.15978  ,  0.46619  , -0.019169 ,\n",
      "        0.41479  , -0.34349  ,  0.26872  ,  0.04464  ,  0.42131  ,\n",
      "       -0.41032  ,  0.15459  ,  0.022239 , -0.64653  ,  0.25256  ,\n",
      "        0.043136 , -0.19445  ,  0.46516  ,  0.45651  ,  0.68588  ,\n",
      "        0.091295 ,  0.21875  , -0.70351  ,  0.16785  , -0.35079  ,\n",
      "       -0.12634  ,  0.66384  , -0.2582   ,  0.036542 , -0.13605  ,\n",
      "        0.40253  ,  0.14289  ,  0.38132  , -0.12283  , -0.45886  ,\n",
      "       -0.25282  , -0.30432  , -0.11215  , -0.26182  , -0.22482  ,\n",
      "       -0.44554  ,  0.2991   , -0.85612  , -0.14503  , -0.49086  ,\n",
      "        0.0082973, -0.17491  ,  0.27524  ,  1.4401   , -0.21239  ,\n",
      "       -2.8435   , -0.27958  , -0.45722  ,  1.6386   ,  0.78808  ,\n",
      "       -0.55262  ,  0.65     ,  0.086426 ,  0.39012  ,  1.0632   ,\n",
      "       -0.35379  ,  0.48328  ,  0.346    ,  0.84174  ,  0.098707 ,\n",
      "       -0.24213  , -0.27053  ,  0.045287 , -0.40147  ,  0.11395  ,\n",
      "        0.0062226,  0.036673 ,  0.018518 , -1.0213   , -0.20806  ,\n",
      "        0.64072  , -0.068763 , -0.58635  ,  0.33476  , -1.1432   ,\n",
      "       -0.1148   , -0.25091  , -0.45907  , -0.096819 , -0.17946  ,\n",
      "       -0.063351 , -0.67412  , -0.068895 ,  0.53604  , -0.87773  ,\n",
      "        0.31802  , -0.39242  , -0.23394  ,  0.47298  , -0.028803 ],\n",
      "      dtype=float32), 'of': array([-0.1529  , -0.24279 ,  0.89837 ,  0.16996 ,  0.53516 ,  0.48784 ,\n",
      "       -0.58826 , -0.17982 , -1.3581  ,  0.42541 ,  0.15377 ,  0.24215 ,\n",
      "        0.13474 ,  0.41193 ,  0.67043 , -0.56418 ,  0.42985 , -0.012183,\n",
      "       -0.11677 ,  0.31781 ,  0.054177, -0.054273,  0.35516 , -0.30241 ,\n",
      "        0.31434 , -0.33846 ,  0.71715 , -0.26855 , -0.15837 , -0.47467 ,\n",
      "        0.051581, -0.33252 ,  0.15003 , -0.1299  , -0.54617 , -0.37843 ,\n",
      "        0.64261 ,  0.82187 , -0.080006,  0.078479, -0.96976 , -0.57741 ,\n",
      "        0.56491 , -0.39873 , -0.057099,  0.19743 ,  0.065706, -0.48092 ,\n",
      "       -0.20125 , -0.40834 ,  0.39456 , -0.02642 , -0.11838 ,  1.012   ,\n",
      "       -0.53171 , -2.7474  , -0.042981, -0.74849 ,  1.7574  ,  0.59085 ,\n",
      "        0.04885 ,  0.78267 ,  0.38497 ,  0.42097 ,  0.67882 ,  0.10337 ,\n",
      "        0.6328  , -0.026595,  0.58647 , -0.44332 ,  0.33057 , -0.12022 ,\n",
      "       -0.55645 ,  0.073611,  0.20915 ,  0.43395 , -0.012761,  0.089874,\n",
      "       -1.7991  ,  0.084808,  0.77112 ,  0.63105 , -0.90685 ,  0.60326 ,\n",
      "       -1.7515  ,  0.18596 , -0.50687 , -0.70203 ,  0.66578 , -0.81304 ,\n",
      "        0.18712 , -0.018488, -0.26757 ,  0.727   , -0.59363 , -0.34839 ,\n",
      "       -0.56094 , -0.591   ,  1.0039  ,  0.20664 ], dtype=float32), 'to': array([-1.8970e-01,  5.0024e-02,  1.9084e-01, -4.9184e-02, -8.9737e-02,\n",
      "        2.1006e-01, -5.4952e-01,  9.8377e-02, -2.0135e-01,  3.4241e-01,\n",
      "       -9.2677e-02,  1.6100e-01, -1.3268e-01, -2.8160e-01,  1.8737e-01,\n",
      "       -4.2959e-01,  9.6039e-01,  1.3972e-01, -1.0781e+00,  4.0518e-01,\n",
      "        5.0539e-01, -5.5064e-01,  4.8440e-01,  3.8044e-01, -2.9055e-03,\n",
      "       -3.4942e-01, -9.9696e-02, -7.8368e-01,  1.0363e+00, -2.3140e-01,\n",
      "       -4.7121e-01,  5.7126e-01, -2.1454e-01,  3.5958e-01, -4.8319e-01,\n",
      "        1.0875e+00,  2.8524e-01,  1.2447e-01, -3.9248e-02, -7.6732e-02,\n",
      "       -7.6343e-01, -3.2409e-01, -5.7490e-01, -1.0893e+00, -4.1811e-01,\n",
      "        4.5120e-01,  1.2112e-01, -5.1367e-01, -1.3349e-01, -1.1378e+00,\n",
      "       -2.8768e-01,  1.6774e-01,  5.5804e-01,  1.5387e+00,  1.8859e-02,\n",
      "       -2.9721e+00, -2.4216e-01, -9.2495e-01,  2.1992e+00,  2.8234e-01,\n",
      "       -3.4780e-01,  5.1621e-01, -4.3387e-01,  3.6852e-01,  7.4573e-01,\n",
      "        7.2102e-02,  2.7931e-01,  9.2569e-01, -5.0336e-02, -8.5856e-01,\n",
      "       -1.3580e-01, -9.2551e-01, -3.3991e-01, -1.0394e+00, -6.7203e-02,\n",
      "       -2.1379e-01, -4.7690e-01,  2.1377e-01, -8.4008e-01,  5.2536e-02,\n",
      "        5.9298e-01,  2.9604e-01, -6.7644e-01,  1.3916e-01, -1.5504e+00,\n",
      "       -2.0765e-01,  7.2220e-01,  5.2056e-01, -7.6221e-02, -1.5194e-01,\n",
      "       -1.3134e-01,  5.8617e-02, -3.1869e-01, -6.1419e-01, -6.2393e-01,\n",
      "       -4.1548e-01, -3.8175e-02, -3.9804e-01,  4.7647e-01, -1.5983e-01],\n",
      "      dtype=float32)}\n",
      "\n",
      "Shape of Embedding Matrix: (15, 100)\n",
      "Sample values from Embedding Matrix:\n",
      " [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.57058001  0.44183001  0.70102    -0.41712999 -0.34057999  0.02339\n",
      "  -0.071537    0.48177001 -0.013121    0.16834    -0.13389     0.040626\n",
      "   0.15827    -0.44341999 -0.019403   -0.009661   -0.046284    0.093228\n",
      "  -0.27331001  0.22849999  0.33089    -0.36474001  0.078741    0.3585\n",
      "   0.44757    -0.2299      0.18076999 -0.62650001  0.053852   -0.29154\n",
      "  -0.42559999  0.62902999  0.14393    -0.046004   -0.21007     0.48879001\n",
      "  -0.057698    0.37430999 -0.030075   -0.34494001 -0.29701999  0.15095\n",
      "   0.28248    -0.16577999  0.076131   -0.093016    0.79364997 -0.60488999\n",
      "  -0.18874    -1.01730001  0.31962001 -0.16344     0.54176998  1.17250001\n",
      "  -0.47874999 -3.3842001  -0.081301   -0.35280001  1.83720005  0.44516\n",
      "  -0.52666003  0.99786001 -0.32178     0.033462    1.17830002 -0.072905\n",
      "   0.39737001  0.26166001  0.33111    -0.35629001 -0.16558    -0.44382\n",
      "  -0.14183    -0.37976     0.28994    -0.029114   -0.35168999 -0.27693999\n",
      "  -1.34399998  0.19554999  0.16887     0.040237   -0.80211997  0.23366\n",
      "  -1.38370001 -0.023132    0.085395   -0.74050999 -0.073934   -0.58837998\n",
      "  -0.085735   -0.10525    -0.51571     0.15038    -0.16694    -0.16372\n",
      "  -0.22702    -0.66101998  0.47196999  0.37253001]\n",
      " [-0.54263997  0.41475999  1.03219998 -0.40244001  0.46691     0.21816\n",
      "  -0.074864    0.47332001  0.080996   -0.22079    -0.12808    -0.1144\n",
      "   0.50891     0.11568     0.028211   -0.3628      0.43823001  0.047511\n",
      "   0.20282     0.49857    -0.10068     0.13269     0.16971999  0.11653\n",
      "   0.31355     0.25713     0.092783   -0.56826001 -0.52974999 -0.051456\n",
      "  -0.67325997  0.92532998  0.26930001  0.22734     0.66364998  0.26221001\n",
      "   0.19719     0.26089999  0.18774    -0.34540001 -0.42635     0.13975\n",
      "   0.56338    -0.56906998  0.12398    -0.12894     0.72483999 -0.26104999\n",
      "  -0.26313999 -0.43605     0.078908   -0.84145999  0.51595002  1.39970005\n",
      "  -0.76459998 -3.14529991 -0.29201999 -0.31246999  1.51289999  0.52434999\n",
      "   0.21456     0.42451999 -0.088411   -0.17805     1.18760002  0.10579\n",
      "   0.76571     0.21913999  0.35824001 -0.11636     0.093261   -0.62483001\n",
      "  -0.21898     0.21796     0.74056    -0.43735     0.14342999  0.14719\n",
      "  -1.16050005 -0.050508    0.12677    -0.014395   -0.98676002 -0.091297\n",
      "  -1.20539999 -0.11974     0.047847   -0.54000998  0.52456999 -0.70963001\n",
      "  -0.32528001 -0.1346     -0.41314     0.33434999 -0.0072412   0.32253\n",
      "  -0.044219   -1.29690003  0.76217002  0.46349001]\n",
      " [-0.27085999  0.044006   -0.02026    -0.17395     0.6444      0.71213001\n",
      "   0.35510001  0.47138    -0.29637     0.54426998 -0.72294003 -0.0047612\n",
      "   0.040611    0.043236    0.29729     0.10725     0.40156001 -0.53662002\n",
      "   0.033382    0.067396    0.64556003 -0.085523    0.14103     0.094539\n",
      "   0.74947    -0.19400001 -0.68739003 -0.41740999 -0.22807001  0.12\n",
      "  -0.48999     0.80944997  0.045138   -0.11898     0.20161     0.39276001\n",
      "  -0.20121001  0.31354001  0.75304002  0.25907001 -0.11566    -0.029319\n",
      "   0.93498999 -0.36067     0.52420002  0.23706     0.52714998  0.22869\n",
      "  -0.51958001 -0.79348999 -0.20367999 -0.50186998  0.18748     0.94282001\n",
      "  -0.44834    -3.67919993  0.044183   -0.26751     2.19970012  0.241\n",
      "  -0.033425    0.69553    -0.64472002 -0.0072277   0.89574999  0.20015\n",
      "   0.46493     0.61932999 -0.1066      0.08691    -0.4623      0.18262\n",
      "  -0.15849     0.020791    0.19373     0.063426   -0.31672999 -0.48177001\n",
      "  -1.38479996  0.13669001  0.96859002  0.049965   -0.27379999 -0.035686\n",
      "  -1.05770004 -0.24467     0.90366    -0.12442     0.080776   -0.83401\n",
      "   0.57200998  0.088945   -0.42532    -0.018253   -0.079995   -0.28580999\n",
      "  -0.01089    -0.4923      0.63687003  0.23642001]\n",
      " [-0.013786    0.38216001  0.53236002  0.15261    -0.29694    -0.20558\n",
      "  -0.41846001 -0.58437002 -0.77354997 -0.87866002 -0.37858    -0.18516\n",
      "  -0.12800001 -0.20584001 -0.22925    -0.42598999  0.3725      0.26076999\n",
      "  -1.07019997  0.62915999 -0.091469    0.70348001 -0.4973     -0.77691001\n",
      "   0.66044998  0.09465    -0.44893     0.018917    0.33146    -0.35021999\n",
      "  -0.35789001  0.030313    0.22253001 -0.23236001 -0.19719    -0.0053125\n",
      "  -0.25848001  0.58081001 -0.10705    -0.17845    -0.16205999  0.087086\n",
      "   0.63028997 -0.76648998  0.51618999  0.14072999  1.01900005 -0.43136001\n",
      "   0.46138    -0.43584999 -0.47567999  0.19226     0.36065     0.78987002\n",
      "   0.088945   -2.78139997 -0.15366     0.01015     1.17980003  0.15167999\n",
      "  -0.050112    1.26259995 -0.77526999  0.36030999  0.95761001 -0.11385\n",
      "   0.28035    -0.02591     0.31246001 -0.15424     0.37779999 -0.13598999\n",
      "   0.29460001 -0.31579     0.42943001  0.086969    0.019169   -0.27241999\n",
      "  -0.31696001  0.37327     0.61997002  0.13889     0.17188001  0.30362999\n",
      "  -1.27760005  0.044423   -0.52736002 -0.88536    -0.19428    -0.61947\n",
      "  -0.10146    -0.26301    -0.061707    0.36627001 -0.95222998 -0.39346001\n",
      "  -0.69182998 -1.04260004  0.28854999  0.63055998]]\n",
      "\n",
      "Shape of y: (3,)\n",
      "Sample values from y:\n",
      " [1 0 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,500</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │           \u001b[38;5;34m1,500\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,500</span> (5.86 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,500\u001b[0m (5.86 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,500</span> (5.86 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,500\u001b[0m (5.86 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Sample DataFrame for demonstration\n",
    "df = pd.DataFrame({\n",
    "    'cleaned_review': [\"This is a great movie\", \"I did not like the film\", \"An excellent watch\"],\n",
    "    'sentiment': [\"pos\", \"neg\", \"pos\"]\n",
    "})\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['cleaned_review'])\n",
    "sequences = tokenizer.texts_to_sequences(df['cleaned_review'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Print some values from tokenizer\n",
    "print(\"Word Index:\\n\", list(word_index.items())[:10])  # Print first 10 word indices\n",
    "\n",
    "# Pad sequences\n",
    "max_length = 100\n",
    "X = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "# Print shape and some values of X\n",
    "print(\"\\nShape of X:\", X.shape)\n",
    "print(\"Sample values from X:\\n\", X[:5])\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "glove_file = r'C:\\Users\\hp\\Documents\\internships\\Ignite\\glove.6B.100d.txt'\n",
    "embeddings_index = load_glove_embeddings(glove_file)\n",
    "\n",
    "# Print some values from embeddings_index\n",
    "print(\"\\nSample GloVe embeddings:\\n\", {k: embeddings_index[k] for k in list(embeddings_index)[:5]})\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Print shape and some values of embedding matrix\n",
    "print(\"\\nShape of Embedding Matrix:\", embedding_matrix.shape)\n",
    "print(\"Sample values from Embedding Matrix:\\n\", embedding_matrix[:5])\n",
    "\n",
    "# Prepare the labels\n",
    "y = df['sentiment'].apply(lambda x: 1 if x == 'pos' else 0).values\n",
    "\n",
    "# Print shape and some values of y\n",
    "print(\"\\nShape of y:\", y.shape)\n",
    "print(\"Sample values from y:\\n\", y[:5])\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word_index) + 1, \n",
    "                    output_dim=embedding_dim, \n",
    "                    weights=[embedding_matrix], \n",
    "                    input_length=max_length, \n",
    "                    trainable=False))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Assuming X and y are your features and labels\n",
    "# model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7840d2-83fc-4da1-ab62-a9f17f3b015a",
   "metadata": {},
   "source": [
    "##### 2.4 Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9a45e96-c976-4af8-b4b7-fbe52aace453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I loved this movie, it was fantastic!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step\n",
      "Sentiment: positive\n",
      "\n",
      "Review: This was a terrible movie, I hated it.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Sentiment: positive\n",
      "\n",
      "Review: It was an average movie, not too bad but not great either.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Sentiment: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(review):\n",
    "    cleaned_review = preprocess_text(review)\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_review])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length)\n",
    "    prediction = model.predict(padded_sequence)[0][0]\n",
    "    return 'positive' if prediction > 0.5 else 'negative'\n",
    "\n",
    "# Example reviews\n",
    "reviews = [\n",
    "    \"I loved this movie, it was fantastic!\",\n",
    "    \"This was a terrible movie, I hated it.\",\n",
    "    \"It was an average movie, not too bad but not great either.\"\n",
    "]\n",
    "\n",
    "for review in reviews:\n",
    "    print(f'Review: {review}')\n",
    "    print(f'Sentiment: {predict_sentiment(review)}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113f0020-42e4-4e9d-91b6-e1e33f7127b0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19211231-af48-4b71-bc40-bb2c61b4bf42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aeae17-2848-40ed-a9f5-ec77b7c5e71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49e3521-4a14-40d9-8d35-7d6859275d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e2cc9f-a8d7-4a9b-8da4-702c9b8ad689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
